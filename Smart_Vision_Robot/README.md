# Smart Vision Robot â€” CNN + LSTM + DQN

Run training:
python training/train_agent.py

Evaluate:
python training/evaluate.py --model models/final_seq_model.keras

Play / demo:
python training/play.py --model models/final_seq_model.keras

# ğŸ¤– Smart Vision Robot: Learning to Navigate Using AI

An autonomous navigation system where an AI agent **learns to navigate complex grid environments** using **Deep Reinforcement Learning (DRL)**.  
It integrates **CNNs (Convolutional Neural Networks)** for visual input processing and **RNNs (LSTM)** for sequential decision-making.

---

## ğŸ¯ Objective
To design an AI system that:
- **Sees** the environment using a camera-like grid processed through CNN layers.  
- **Thinks** using RNN (LSTM) to remember past movements.  
- **Learns** optimal navigation through **Q-learning (Deep Q-Network)**.  

---

## ğŸ§  Technologies & Concepts Used

| Technology | Purpose | Why Used |
|-------------|----------|-----------|
| **Python 3** | Core programming language | Easy integration with ML libraries |
| **TensorFlow / Keras** | Deep learning model building | Supports CNN, LSTM, and Reinforcement Learning |
| **OpenCV** | Video capture and rendering | Used for saving evaluation videos |
| **NumPy / Matplotlib** | Data handling and visualization | Efficient computation & result plotting |
| **Deep Reinforcement Learning (DQN)** | Agent training | Learns optimal actions via reward feedback |
| **Experience Replay** | Memory buffer for Q-learning | Stabilizes training |
| **Double DQN + Dueling Architecture** | Enhanced learning efficiency | Improves accuracy and stability |

---

Trained Model
      â”‚
      â–¼
  Load Environment
      â”‚
      â–¼
  Predict Best Actions
      â”‚
      â–¼
  Move Agent â†’ Collect Reward
      â”‚
      â–¼
  Save Video (Success / Failure)
      â”‚
      â–¼
  Compute Success Rate

## ğŸ“ Project Structure

smart-vision-robot/
â”‚
â”œâ”€â”€ env/
â”‚ â””â”€â”€ simulation_env.py # Environment setup (Grid, rewards, agent moves)
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ cnn_lstm_model.py # CNN + LSTM (Dueling DQN) model architecture
â”‚ â”œâ”€â”€ replay_buffer.py # Experience Replay memory buffer
â”‚ â””â”€â”€ agent.py # Agent logic: choose action, learn, update Q-network
â”‚
â”œâ”€â”€ training/
â”‚ â””â”€â”€ train_agent.py # Main training script for the agent
â”‚
â”œâ”€â”€ evaluation/
â”‚ â””â”€â”€ evaluate_agent.py # Runs trained agent & saves evaluation videos
â”‚
â”œâ”€â”€ videos/ # Folder to store output evaluation videos
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ summary.md # Project summary and documentation
---

## âš™ï¸ How It Works â€” Flow Explanation

### 1. Environment Setup (`simulation_env.py`)
- Creates a **grid world** where:
  - Green cell = goal (reward +10)  
  - Red cell = obstacle (penalty -5)  
  - Empty cell = free path (reward -0.1)
- The agent receives visual input (grid image) and moves (Up/Down/Left/Right).

### 2. Agent Creation (`agent.py`)
- Uses the **CNN+LSTM Dueling DQN model** from `cnn_lstm_model.py`.
- Maintains:
  - **Main Network** â€“ Learns from experiences.
  - **Target Network** â€“ Provides stable Q-value estimation.
- Uses **epsilon-greedy** strategy:
  - Random exploration initially.
  - Gradually shifts to exploitation (learned decisions).

### 3. Model Architecture (`cnn_lstm_model.py`)
- **CNN Layers:** Extract spatial features (like vision).
- **LSTM Layer:** Retains memory of previous frames.
- **Dueling DQN Output:** Separates *Value* and *Advantage* streams to improve stability.
- **Output Layer:** Predicts Q-values for each possible action.

### 4. Training Process (`train_agent.py`)
1. Initialize environment and networks.
2. Run for multiple **episodes** (e.g., 500â€“2000).  
3. For each step:
   - Get state â†’ predict action â†’ move â†’ get reward â†’ store in replay buffer.
4. Train the model by sampling from memory.
5. Update target network periodically.
6. Save model checkpoints after every few episodes.

ğŸ§© Optimizations used:
- **Double DQN:** Prevents overestimation of Q-values.
- **Dueling Architecture:** Separates state value and action advantage.
- **Soft Updates:** Smooth synchronization between main and target networks.
- **Batch Normalization + Dropout:** Regularization for faster convergence.

---

## ğŸ¥ Evaluation Process (`evaluate_agent.py`)
- Loads the trained model and runs multiple test episodes.
- Saves **videos** of both successful and failed runs:
  - Successful runs (agent reaches goal) â†’ `videos/success_episode_X.mp4`
  - Failed runs â†’ `videos/failure_episode_X.mp4`
- Each video includes:
  - Overlaid episode number and total reward.
  - Grid movement visualization.

---

## ğŸ§© How the Model Learns
1. **Input:** Grid image (state)
2. **Processing:**
   - CNN extracts features (like edges, goal position)
   - LSTM remembers past steps
3. **Output:** Q-values for each possible action
4. **Q-Learning Update Rule:**
   \[
   Q(s, a) = r + \gamma \cdot \max_{a'} Q'(s', a')
   \]
5. **Experience Replay:** Random batches improve stability
6. **Target Network:** Updated softly every few steps to reduce oscillation

---

| Step | Component          | Description                                     |
| ---- | ------------------ | ----------------------------------------------- |
| 1    | **Environment**    | Generates visual grid world                     |
| 2    | **CNN**            | Extracts visual spatial features                |
| 3    | **LSTM**           | Remembers past steps (temporal info)            |
| 4    | **Dueling DQN**    | Learns Q-values using state & action advantages |
| 5    | **Replay Buffer**  | Stores experience for stable training           |
| 6    | **Q-Update Loop**  | Learns from sampled experiences                 |
| 7    | **Target Network** | Provides stable learning targets                |
| 8    | **Evaluation**     | Measures success and saves videos               |

config.py      â†’ defines hyperparameters
train_agent.py â†’ runs full training loop
env/           â†’ provides world + state feedback
agents/        â†’ defines learning model and replay buffer
models/        â†’ saves trained CNN+LSTM DQN
evaluate_*.py  â†’ tests model and records navigation videos


| Technology               | Use                           | Why                                        |
| ------------------------ | ----------------------------- | ------------------------------------------ |
| **TensorFlow / Keras**   | Building CNN-LSTM DQN models  | Fast GPU training, simple model definition |
| **OpenCV**               | Video rendering and recording | Handles image processing and MP4 writing   |
| **NumPy**                | Array operations              | Efficient numerical computations           |
| **Gym-like Environment** | Simulated world               | Controlled training environment for RL     |
| **TensorBoard**          | Logging and visualization     | Real-time tracking of rewards and losses   |


| Phase           | Mechanism         | Explanation                                                                 |
| --------------- | ----------------- | --------------------------------------------------------------------------- |
| Perception      | CNN               | Learns spatial patterns from the environment (like visual features).        |
| Memory          | LSTM              | Keeps a short-term memory of recent frames â†’ handles partial observability. |
| Decision Making | DQN               | Maps visual sequences to action-value pairs (Q-values).                     |
| Stability       | Target Network    | Provides stable Q-value targets to prevent divergence.                      |
| Replay          | Experience Replay | Reuses past experiences to improve sample efficiency.                       |


## ğŸ“ˆ Training Improvements
- Uses **Adam optimizer** with LR decay.
- Reward shaping encourages reaching the goal efficiently.
- Early stopping if high success rate achieved.

---

## â±ï¸ Training Time
| Setup | Approx Time | Success Rate |
|--------|--------------|--------------|
| CPU (Intel i7) | ~6â€“8 hrs | 80â€“85% |
| GPU (RTX 3060) | ~1.5 hrs | 90â€“95% |

*(depends on grid size, episodes, and replay memory size)*

---
âœ… Results & Insights

The agent learns to reach the goal with minimal collisions.

CNN extracts spatial features effectively.

LSTM improves decision-making by remembering paths.

Double DQN reduces instability and overfitting.

ğŸ“œ Future Enhancements

Add real camera feed input.

Use PPO (Proximal Policy Optimization) for continuous control.

Extend to 3D environment (Unity MLAgents / Gazebo).


                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚         Environment          â”‚
                â”‚ (Grid world, obstacles, goal)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     Visual Observation    â”‚
                 â”‚ (Grid image as input)     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚       CNN Layers          â”‚
                 â”‚ Feature extraction from   â”‚
                 â”‚ environment visuals       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚        LSTM Layer         â”‚
                 â”‚ Learns temporal patterns  â”‚
                 â”‚ (memory of past states)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Dueling DQN Head       â”‚
                 â”‚ - Value Stream (V(s))    â”‚
                 â”‚ - Advantage Stream (A(s,a)) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Predicted Q-values       â”‚
                 â”‚ for all actions          â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Action Selection         â”‚
                 â”‚ (Îµ-greedy policy)        â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Execute Action in Env     â”‚
                 â”‚ â†’ New state, reward       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Store Experience in      â”‚
                 â”‚  Replay Buffer (s, a, r, s')â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Sample Batch & Train DQN  â”‚
                 â”‚ Update Q-network weights  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Soft Update Target Net    â”‚
                 â”‚ for stable learning       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Next Episode Begins     â”‚
                 â”‚   Repeat until convergenceâ”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
